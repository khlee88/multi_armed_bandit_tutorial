# multi armed bandit tutorial

1. Bandit Problem
2. Toy DataSet
3. Random Selection
4. K exploration & N-K exploitation
5. Epsilon-Greedy
6. UCB (Upper Confidence Bound)
7. Thomson Sampling