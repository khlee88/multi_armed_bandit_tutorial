# multi armed bandit tutorial
html page: https://khlee88.github.io/multi_armed_bandit_tutorial/

1. Bandit Problem
2. Toy DataSet
3. Random Selection
4. K exploration & N-K exploitation
5. Epsilon-Greedy
6. UCB (Upper Confidence Bound)
7. Thomson Sampling
